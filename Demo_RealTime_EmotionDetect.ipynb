{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\I011176\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "0",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 0\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct\n",
    "from moviepy.editor import AudioFileClip, AudioFileClip\n",
    "from decord import VideoReader\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "#from The `keras.layers` module in the code snippet you provided is importing layers from the Keras library. Keras is a high-level neural networks API, written in Python and capable of running on top of TensorFlow, CNTK, or Theano.\n",
    "from keras.layers import TFSMLayer\n",
    "from keras import Model, Input\n",
    "from PyQt5.QtWidgets import (\n",
    "    QApplication, QWidget, QLabel, QPushButton,\n",
    "    QFileDialog, QVBoxLayout, QHBoxLayout\n",
    ")\n",
    "from PyQt5.QtCore import Qt, QUrl\n",
    "from PyQt5.QtMultimedia import QMediaPlayer, QMediaContent\n",
    "from PyQt5.QtMultimediaWidgets import QVideoWidget\n",
    "\n",
    "# Settings\n",
    "input_size = 224\n",
    "num_frame = 8\n",
    "sampling_rate = 6\n",
    "\n",
    "# Load model\n",
    "model_path = \"D:/GUI-Emotion-Detection/saved_model\"\n",
    "input_tensor = Input(shape=(None, 224, 224, 3))\n",
    "tfsm_layer = TFSMLayer(model_path, call_endpoint='serving_default')\n",
    "model = Model(inputs=input_tensor, outputs=tfsm_layer(input_tensor))\n",
    "\n",
    "uc_id2label = {0: 'anger', 1: 'happiness', 2: 'surprise', 3: 'disgust', 4: 'fear', 5: 'sadness'}\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "def MFCC(signal, sample_rate):\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(emphasized_signal)\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    NFFT = 512\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = (1.0 / NFFT) * ((mag_frames) ** 2)\n",
    "    nfilt = 40\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = 2595 * np.log10(1 + (sample_rate / 2) / 700)\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = 700 * (10 ** (mel_points / 2595) - 1)\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])\n",
    "        f_m = int(bin[m])\n",
    "        f_m_plus = int(bin[m + 1])\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1:14]\n",
    "    return mfcc\n",
    "\n",
    "def process_video(input_video_path, output_video_path):\n",
    "    cap = cv2.VideoCapture(input_video_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    out = cv2.VideoWriter(output_video_path, fourcc, 16.0, (224, 224))\n",
    "\n",
    "    audio_clip = AudioFileClip(input_video_path)\n",
    "    temp_wav = \"temp.wav\"\n",
    "    audio_clip.write_audiofile(temp_wav, verbose=False, logger=None)\n",
    "    fs, Audiodata = wavfile.read(temp_wav)\n",
    "    Audiodata = normalize_audio(Audiodata)\n",
    "\n",
    "    step = int(len(Audiodata) / 9)\n",
    "    tx = np.arange(0, len(Audiodata), step)\n",
    "\n",
    "    for i in range(8):\n",
    "        signal = Audiodata[tx[i]:tx[i + 1]]\n",
    "        mfcc = MFCC(signal, fs)\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.matshow(np.transpose(mfcc), interpolation=\"nearest\", aspect=\"auto\", origin=\"lower\")\n",
    "        plt.axis('off')\n",
    "        plt.savefig(\"mfcc_temp.jpg\")\n",
    "        plt.close()\n",
    "        img = Image.open(\"mfcc_temp.jpg\").resize((224, 224))\n",
    "        out.write(np.array(img))\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < 8:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    os.remove(\"mfcc_temp.jpg\")\n",
    "    os.remove(temp_wav)\n",
    "    return output_video_path\n",
    "\n",
    "def read_video(file_path):\n",
    "    vr = VideoReader(file_path)\n",
    "    frames = vr.get_batch(range(len(vr))).asnumpy()\n",
    "    return format_frames(frames, output_size=(224, 224))\n",
    "\n",
    "def format_frames(frames, output_size):\n",
    "    frames = tf.image.convert_image_dtype(frames, tf.float32)\n",
    "    frames = tf.image.resize(frames, size=list(output_size))\n",
    "    return frames\n",
    "\n",
    "def load_video(file_path):\n",
    "    video = read_video(file_path)\n",
    "    video = tf.convert_to_tensor(video, dtype=tf.float32)\n",
    "    video = tf.expand_dims(video, axis=0)\n",
    "    return video\n",
    "\n",
    "class VideoProcessorApp(QWidget):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.setWindowTitle(\"Real-time Emotion Detection\")\n",
    "        self.resize(600, 400)\n",
    "        self.setStyleSheet(\"background-color : #3399FF\")\n",
    "\n",
    "        layout = QVBoxLayout()\n",
    "\n",
    "        self.label = QLabel(\n",
    "            \"Welcome to the Real-Time Emotion Detection System. You can either upload a video file to analyze emotions or activate your camera for live emotion detection\"\n",
    "        )\n",
    "        self.label.setAlignment(Qt.AlignCenter)\n",
    "        self.label.setWordWrap(True)\n",
    "        self.label.setStyleSheet(\"font-size: 16px; font-weight: bold; color: #001f3f;\")\n",
    "        layout.addWidget(self.label)\n",
    "\n",
    "        btn_layout = QHBoxLayout()\n",
    "\n",
    "        choose_layout = QVBoxLayout()\n",
    "        self.btn = QPushButton(\"Choose Video\")\n",
    "        self.btn.clicked.connect(self.load_video)\n",
    "        self.btn.setFixedSize(180, 50)\n",
    "        self.btn.setStyleSheet(\n",
    "            \"background-color: #001f3f; color: white; font-size: 15px; font-weight: bold;\"\n",
    "        )\n",
    "        choose_layout.addWidget(self.btn)\n",
    "        choose_layout.addSpacing(20)\n",
    "\n",
    "        self.extra_btn1 = QPushButton(\"Launch Camera\")\n",
    "        self.extra_btn1.setFixedSize(180, 50)\n",
    "        self.extra_btn1.setStyleSheet(\n",
    "            \"background-color: #001f3f; color: white; font-size: 15px; font-weight: bold;\"\n",
    "        )\n",
    "        self.extra_btn1.clicked.connect(self.capture_from_camera)\n",
    "        choose_layout.addWidget(self.extra_btn1)\n",
    "\n",
    "        detect_layout = QVBoxLayout()\n",
    "        self.detect_btn = QPushButton(\"Detect Emotion\")\n",
    "        self.detect_btn.clicked.connect(self.detect_emotion)\n",
    "        self.detect_btn.setFixedSize(220, 50)\n",
    "        self.detect_btn.setStyleSheet(\n",
    "            \"background-color: #001f3f; color: white; font-size: 15px; font-weight: bold;\"\n",
    "        )\n",
    "        detect_layout.addWidget(self.detect_btn)\n",
    "        detect_layout.addSpacing(20)\n",
    "\n",
    "        self.extra_btn2 = QPushButton(\"Detect Emotion from Camera\")\n",
    "        self.extra_btn2.setFixedSize(220, 50)\n",
    "        self.extra_btn2.setStyleSheet(\n",
    "            \"background-color: #001f3f; color: white; font-size: 15px; font-weight: bold;\"\n",
    "        )\n",
    "        self.extra_btn2.clicked.connect(self.capture_from_camera)\n",
    "        detect_layout.addWidget(self.extra_btn2)\n",
    "\n",
    "        btn_layout.addLayout(choose_layout)\n",
    "        btn_layout.addSpacing(100)\n",
    "        btn_layout.addLayout(detect_layout)\n",
    "\n",
    "        layout.addLayout(btn_layout)\n",
    "\n",
    "        self.emotion_label = QLabel(\"\")\n",
    "        self.emotion_label.setAlignment(Qt.AlignCenter)\n",
    "        self.emotion_label.setStyleSheet(\"font-size: 20px; font-weight: bold; color: white;\")\n",
    "        layout.addWidget(self.emotion_label)\n",
    "\n",
    "        self.video_widget = QVideoWidget()\n",
    "        layout.addWidget(self.video_widget)\n",
    "\n",
    "        self.setLayout(layout)\n",
    "\n",
    "        self.media_player = QMediaPlayer(None, QMediaPlayer.VideoSurface)\n",
    "        self.media_player.setVideoOutput(self.video_widget)\n",
    "        self.processed_video_path = \"\"\n",
    "\n",
    "    def load_video(self):\n",
    "        input_dir = r\"D:/GUI-Emotion-Detection/input_videos\"\n",
    "        output_dir = r\"D:/GUI-Emotion-Detection/output_videos\"\n",
    "\n",
    "        file_name, _ = QFileDialog.getOpenFileName(self, \"Open AVI File\", \"\", \"AVI Files (*.avi)\")\n",
    "        if file_name:\n",
    "            base_name = os.path.basename(file_name)\n",
    "            input_path = os.path.join(input_dir, base_name)\n",
    "\n",
    "            os.makedirs(input_dir, exist_ok=True)\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "            if not os.path.exists(input_path):\n",
    "                shutil.copy(file_name, input_path)\n",
    "\n",
    "            output_path = os.path.join(output_dir, base_name.replace('.avi', '_processed.mp4'))\n",
    "            self.processed_video_path = process_video(input_path, output_path)\n",
    "\n",
    "            self.label.setText(f\"Processed: {base_name}\")\n",
    "            self.media_player.setMedia(QMediaContent(QUrl.fromLocalFile(self.processed_video_path)))\n",
    "            self.media_player.play()\n",
    "            self.emotion_label.setText(\"\")\n",
    "\n",
    "    def detect_emotion(self):\n",
    "        if self.processed_video_path:\n",
    "            video_tensor = load_video(self.processed_video_path)\n",
    "            prediction = model.predict(video_tensor)\n",
    "            predicted_label = uc_id2label[np.argmax(prediction)]\n",
    "            self.emotion_label.setText(f\"Emotion Detected: {predicted_label}\")\n",
    "\n",
    "    def capture_from_camera(self):\n",
    "        input_dir = \"D:/GUI-Emotion-Detection/input_videos_camera\"\n",
    "        output_dir = \"D:/GUI-Emotion-Detection/output_videos_camera\"\n",
    "        os.makedirs(input_dir, exist_ok=True)\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "        input_path = os.path.join(input_dir, \"camera_input.avi\")\n",
    "        output_path = os.path.join(output_dir, \"camera_output_processed.mp4\")\n",
    "\n",
    "        cap = cv2.VideoCapture(0)\n",
    "        fourcc = cv2.VideoWriter_fourcc(*'XVID')\n",
    "        out = cv2.VideoWriter(input_path, fourcc, 16.0, (224, 480))  # 16 FPS\n",
    "\n",
    "        frame_count = 0\n",
    "        max_frames = 32  # 2 seconds at 16 FPS\n",
    "\n",
    "        while cap.isOpened() and frame_count < max_frames:\n",
    "            ret, frame = cap.read()\n",
    "            if not ret:\n",
    "                break\n",
    "            out.write(frame)\n",
    "            frame_count += 1\n",
    "            cv2.imshow(\"Recording from Camera\", frame)\n",
    "            if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "               break\n",
    "\n",
    "        cap.release()\n",
    "        out.release()\n",
    "        cv2.destroyAllWindows()\n",
    "\n",
    "        # 🔁 SAME preprocessing pipeline\n",
    "        processed_path = process_video(input_path, output_path)\n",
    "        self.processed_video_path = processed_path\n",
    "        self.media_player.setMedia(QMediaContent(QUrl.fromLocalFile(processed_path)))\n",
    "        self.media_player.play()\n",
    "\n",
    "        # 🔍 Emotion detection\n",
    "        video_tensor = load_video(processed_path)  # Must be identical to load_video() in load_video()\n",
    "        prediction = model.predict(video_tensor)\n",
    "        predicted_label = uc_id2label[np.argmax(prediction)]\n",
    "        self.emotion_label.setText(f\"Detected Emotion: {predicted_label}\")\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    app = QApplication(sys.argv)\n",
    "    window = VideoProcessorApp()\n",
    "    window.show()\n",
    "    sys.exit(app.exec_())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Retrying (Retry(total=4, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Eine vorhandene Verbindung wurde vom Remotehost geschlossen', None, 10054, None))': /simple/streamlit/\n",
      "WARNING: Retrying (Retry(total=3, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Eine vorhandene Verbindung wurde vom Remotehost geschlossen', None, 10054, None))': /simple/streamlit/\n",
      "WARNING: Retrying (Retry(total=2, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Eine vorhandene Verbindung wurde vom Remotehost geschlossen', None, 10054, None))': /simple/streamlit/\n",
      "WARNING: Retrying (Retry(total=1, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Eine vorhandene Verbindung wurde vom Remotehost geschlossen', None, 10054, None))': /simple/streamlit/\n",
      "WARNING: Retrying (Retry(total=0, connect=None, read=None, redirect=None, status=None)) after connection broken by 'ProtocolError('Connection aborted.', ConnectionResetError(10054, 'Eine vorhandene Verbindung wurde vom Remotehost geschlossen', None, 10054, None))': /simple/streamlit/\n",
      "ERROR: Could not find a version that satisfies the requirement streamlit (from versions: none)\n",
      "ERROR: No matching distribution found for streamlit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "!pip install streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import numpy as np\n",
    "import streamlit as st\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from scipy.io import wavfile\n",
    "from scipy.fftpack import dct\n",
    "from moviepy.editor import AudioFileClip\n",
    "import tempfile\n",
    "import tensorflow as tf\n",
    "from keras.layers import Input\n",
    "from keras import Model\n",
    "from keras.layers import TFSMLayer\n",
    "from decord import VideoReader\n",
    "\n",
    "# Constants\n",
    "input_size = 224\n",
    "num_frame = 8\n",
    "sampling_rate = 6\n",
    "\n",
    "# Load model\n",
    "model_path = '/home/jovyan/first-testing-workspace-upd/Emotion-recognition/Models/saved_model'\n",
    "input_tensor = Input(shape=(None, 224, 224, 3))\n",
    "tfsm_layer = TFSMLayer(model_path, call_endpoint='serving_default')\n",
    "model = Model(inputs=input_tensor, outputs=tfsm_layer(input_tensor))\n",
    "\n",
    "uc_id2label = {0: 'anger', 1: 'happiness', 2: 'surprise', 3: 'disgust', 4: 'fear', 5: 'sadness'}\n",
    "\n",
    "def normalize_audio(audio):\n",
    "    return audio / np.max(np.abs(audio))\n",
    "\n",
    "def MFCC(signal, sample_rate):\n",
    "    pre_emphasis = 0.97\n",
    "    emphasized_signal = np.append(signal[0], signal[1:] - pre_emphasis * signal[:-1])\n",
    "    frame_size = 0.025\n",
    "    frame_stride = 0.01\n",
    "    frame_length, frame_step = int(round(frame_size * sample_rate)), int(round(frame_stride * sample_rate))\n",
    "    signal_length = len(emphasized_signal)\n",
    "    num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "    pad_signal_length = num_frames * frame_step + frame_length\n",
    "    z = np.zeros((pad_signal_length - signal_length))\n",
    "    pad_signal = np.append(emphasized_signal, z)\n",
    "    indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + \\\n",
    "              np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "    frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "    frames *= np.hamming(frame_length)\n",
    "    NFFT = 512\n",
    "    mag_frames = np.absolute(np.fft.rfft(frames, NFFT))\n",
    "    pow_frames = (1.0 / NFFT) * ((mag_frames) ** 2)\n",
    "    nfilt = 40\n",
    "    low_freq_mel = 0\n",
    "    high_freq_mel = 2595 * np.log10(1 + (sample_rate / 2) / 700)\n",
    "    mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)\n",
    "    hz_points = 700 * (10 ** (mel_points / 2595) - 1)\n",
    "    bin = np.floor((NFFT + 1) * hz_points / sample_rate)\n",
    "    fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "    for m in range(1, nfilt + 1):\n",
    "        f_m_minus = int(bin[m - 1])\n",
    "        f_m = int(bin[m])\n",
    "        f_m_plus = int(bin[m + 1])\n",
    "        for k in range(f_m_minus, f_m):\n",
    "            fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "        for k in range(f_m, f_m_plus):\n",
    "            fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "    filter_banks = np.dot(pow_frames, fbank.T)\n",
    "    filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)\n",
    "    filter_banks = 20 * np.log10(filter_banks)\n",
    "    mfcc = dct(filter_banks, type=2, axis=1, norm='ortho')[:, 1:14]\n",
    "    return mfcc\n",
    "\n",
    "def process_video(input_path):\n",
    "    cap = cv2.VideoCapture(input_path)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "    temp_output = tempfile.NamedTemporaryFile(delete=False, suffix=\".mp4\").name\n",
    "    out = cv2.VideoWriter(temp_output, fourcc, 16.0, (224, 224))\n",
    "\n",
    "    audio_clip = AudioFileClip(input_path)\n",
    "    temp_wav = tempfile.NamedTemporaryFile(delete=False, suffix=\".wav\").name\n",
    "    audio_clip.write_audiofile(temp_wav, verbose=False, logger=None)\n",
    "    fs, Audiodata = wavfile.read(temp_wav)\n",
    "    Audiodata = normalize_audio(Audiodata)\n",
    "\n",
    "    step = int(len(Audiodata) / 9)\n",
    "    tx = np.arange(0, len(Audiodata), step)\n",
    "\n",
    "    for i in range(8):\n",
    "        signal = Audiodata[tx[i]:tx[i + 1]]\n",
    "        mfcc = MFCC(signal, fs)\n",
    "        fig, ax = plt.subplots(figsize=(4, 4))\n",
    "        ax.matshow(np.transpose(mfcc), interpolation=\"nearest\", aspect=\"auto\", origin=\"lower\")\n",
    "        plt.axis('off')\n",
    "        temp_img = tempfile.NamedTemporaryFile(delete=False, suffix=\".jpg\").name\n",
    "        plt.savefig(temp_img)\n",
    "        plt.close()\n",
    "        img = Image.open(temp_img).resize((224, 224))\n",
    "        out.write(np.array(img))\n",
    "        os.remove(temp_img)\n",
    "\n",
    "    frame_count = 0\n",
    "    while cap.isOpened() and frame_count < 8:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frame = cv2.resize(frame, (224, 224))\n",
    "        out.write(frame)\n",
    "        frame_count += 1\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    os.remove(temp_wav)\n",
    "    return temp_output\n",
    "\n",
    "def read_video(file_path):\n",
    "    vr = VideoReader(file_path)\n",
    "    frames = vr.get_batch(range(len(vr))).asnumpy()\n",
    "    return format_frames(frames, output_size=(224, 224))\n",
    "\n",
    "def format_frames(frames, output_size):\n",
    "    frames = tf.image.convert_image_dtype(frames, tf.float32)\n",
    "    frames = tf.image.resize(frames, size=list(output_size))\n",
    "    return frames\n",
    "\n",
    "def load_video_tensor(file_path):\n",
    "    video = read_video(file_path)\n",
    "    video = tf.convert_to_tensor(video, dtype=tf.float32)\n",
    "    video = tf.expand_dims(video, axis=0)\n",
    "    return video\n",
    "\n",
    "def predict_emotion(video_tensor):\n",
    "    predictions = model(video_tensor)\n",
    "    predicted_label = tf.argmax(predictions, axis=-1).numpy()[0]\n",
    "    return uc_id2label[predicted_label]\n",
    "\n",
    "# === Streamlit Interface ===\n",
    "st.set_page_config(page_title=\"🎥 Real-Time Emotion Detection\", layout=\"centered\")\n",
    "st.title(\"🎥 Real-Time Emotion Detection System\")\n",
    "st.markdown(\"Upload a video file (preferably `.avi`) to detect emotions.\")\n",
    "\n",
    "uploaded_file = st.file_uploader(\"📁 Upload a video\", type=[\"avi\", \"mp4\", \"mov\"])\n",
    "\n",
    "if uploaded_file:\n",
    "    temp_input_path = tempfile.NamedTemporaryFile(delete=False, suffix=\".avi\").name\n",
    "    with open(temp_input_path, \"wb\") as f:\n",
    "        f.write(uploaded_file.read())\n",
    "\n",
    "    st.info(\"🔄 Processing video and extracting MFCC + frames...\")\n",
    "    processed_path = process_video(temp_input_path)\n",
    "\n",
    "    st.video(processed_path)\n",
    "\n",
    "    if st.button(\"🧠 Detect Emotion\"):\n",
    "        st.info(\"Running inference...\")\n",
    "        video_tensor = load_video_tensor(processed_path)\n",
    "        emotion = predict_emotion(video_tensor)\n",
    "        st.success(f\"Detected Emotion: **{emotion.upper()}**\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
